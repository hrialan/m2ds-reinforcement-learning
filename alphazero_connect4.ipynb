{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f84f41a1",
   "metadata": {},
   "source": [
    "This notebook contains everything needed to run our algorithm.\n",
    "You have to launch all the import/classes at the top, then there is a cell to test model against each others and one cell\n",
    "to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0f9a3cc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0f9a3cc",
    "outputId": "bb628af6-d238-4284-e2ba-a239c488fd75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras import Input\n",
    "from keras.layers import Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb86dbb",
   "metadata": {},
   "source": [
    "Implémentation de puissance 4. Code provenant de https://github.com/IASIAI/gym-connect-four/blob/master/gym_connect_four/envs/connect_four_env.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2f230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pygame\n",
    "#!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "528861b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "528861b6",
    "outputId": "7a358cfa-abe2-4942-f533-f026657ae6e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "#connectfour implementation using gym. taken from https://github.com/IASIAI/gym-connect-four/blob/master/gym_connect_four/envs/connect_four_env.py\n",
    "import random\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import deque\n",
    "from enum import Enum, unique\n",
    "from operator import itemgetter\n",
    "from typing import Tuple, NamedTuple, Hashable, Optional\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pygame\n",
    "from gym import error\n",
    "from gym import spaces\n",
    "#from keras.engine.saving import load_model\n",
    "\n",
    "#unique\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "class Color(object):\n",
    "    WHITE = (255, 255, 255)\n",
    "    RED = (255, 0, 0)\n",
    "    BLUE = (0, 0, 255)\n",
    "    YELLOW = (255, 255, 0)\n",
    "\n",
    "def render_board(board,\n",
    "                 image_width=512,\n",
    "                 image_height=512,\n",
    "                 board_percent_x=0.8,\n",
    "                 board_percent_y=0.8,\n",
    "                 items_padding_x=0.05,\n",
    "                 items_padding_y=0.05,\n",
    "                 slot_padding_x=0.1,\n",
    "                 slot_padding_y=0.1,\n",
    "                 background_color=Color.WHITE,\n",
    "                 board_color=Color.BLUE,\n",
    "                 empty_slot_color=Color.WHITE,\n",
    "                 player1_slot_color=Color.RED,\n",
    "                 player2_slot_color=Color.YELLOW):\n",
    "    image = Image.new('RGB', (image_height, image_width), background_color)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    board_width = int(image_width * board_percent_x)\n",
    "    board_height = int(image_height * board_percent_y)\n",
    "\n",
    "    padding_x = image_width - board_width\n",
    "    padding_y = image_height - board_height\n",
    "\n",
    "    padding_top = padding_y // 2\n",
    "    padding_bottom = padding_y - padding_top\n",
    "\n",
    "    padding_left = padding_x // 2\n",
    "    padding_right = padding_x - padding_left\n",
    "\n",
    "    draw.rectangle([\n",
    "        (padding_left, padding_top),\n",
    "        (image_width - padding_right, image_height - padding_bottom)\n",
    "    ], fill=board_color)\n",
    "\n",
    "    padding_left += int(items_padding_x * image_width)\n",
    "    padding_right += int(items_padding_x * image_width)\n",
    "\n",
    "    padding_top += int(items_padding_y * image_height)\n",
    "    padding_bottom += int(items_padding_y * image_height)\n",
    "\n",
    "    cage_width = int((image_width - padding_left - padding_right) / board.shape[1])\n",
    "    cage_height = int((image_width - padding_top - padding_bottom) / board.shape[0])\n",
    "\n",
    "    radius_x = int((cage_width - 2 * int(cage_width * slot_padding_x)) // 2)\n",
    "    radius_y = int((cage_height - 2 * int(cage_height * slot_padding_y)) // 2)\n",
    "\n",
    "    slots = []\n",
    "    for row in range(board.shape[0]):\n",
    "        for column in range(board.shape[1]):\n",
    "            player = board[row, column]\n",
    "\n",
    "            actual_row = board.shape[0] - row - 1\n",
    "            origin_x = padding_left + int(column * cage_width + cage_width // 2)\n",
    "            origin_y = padding_top + int(actual_row * cage_height + cage_height // 2)\n",
    "\n",
    "            slots.append((origin_x, origin_y, player))\n",
    "\n",
    "    for origin_x, origin_y, player in slots:\n",
    "        color = empty_slot_color\n",
    "        if player == 1:\n",
    "            color = player1_slot_color\n",
    "        elif player == -1:\n",
    "            color = player2_slot_color\n",
    "\n",
    "        draw.ellipse([\n",
    "            (origin_x - radius_x, origin_y - radius_y),\n",
    "            (origin_x + radius_x, origin_y + radius_y)\n",
    "        ], fill=color)\n",
    "\n",
    "    return np.array(image)\n",
    "class ResultType(Enum):\n",
    "    NONE = None\n",
    "    DRAW = 0\n",
    "    WIN1 = 1\n",
    "    WIN2 = -1\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        \"\"\"\n",
    "        Need to implement this due to an unfixed bug in Python since 2017: https://bugs.python.org/issue30545\n",
    "        \"\"\"\n",
    "        return self.value == other.value\n",
    "\n",
    "\n",
    "class ConnectFourEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        ConnectFour game environment\n",
    "    Observation:\n",
    "        Type: Discreet(6,7)\n",
    "    Actions:\n",
    "        Type: Discreet(7)\n",
    "        Num     Action\n",
    "        x       Column in which to insert next token (0-6)\n",
    "    Reward:\n",
    "        Reward is 0 for every step.\n",
    "        If there are no other further steps possible, Reward is 0.5 and termination will occur\n",
    "        If it's a win condition, Reward will be 1 and termination will occur\n",
    "        If it is an invalid move, Reward will be -1 and termination will occur\n",
    "    Starting State:\n",
    "        All observations are assigned a value of 0\n",
    "    Episode Termination:\n",
    "        No more spaces left for pieces\n",
    "        4 pieces are present in a line: horizontal, vertical or diagonally\n",
    "        An attempt is made to place a piece in an invalid location\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    LOSS_REWARD = -1\n",
    "    DEF_REWARD = 0\n",
    "    DRAW_REWARD = 0\n",
    "    WIN_REWARD = 1\n",
    "\n",
    "    class StepResult(NamedTuple):\n",
    "\n",
    "        res_type: ResultType\n",
    "\n",
    "        def get_reward(self, player: int):\n",
    "            if self.res_type is ResultType.NONE:\n",
    "                return ConnectFourEnv.DEF_REWARD\n",
    "            elif self.res_type is ResultType.DRAW:\n",
    "                return ConnectFourEnv.DRAW_REWARD\n",
    "            else:\n",
    "                return {ResultType.WIN1.value: ConnectFourEnv.WIN_REWARD, ResultType.WIN2.value: ConnectFourEnv.LOSS_REWARD}[\n",
    "                    self.res_type.value * player]\n",
    "\n",
    "        def is_done(self):\n",
    "            return self.res_type != ResultType.NONE\n",
    "\n",
    "    def __init__(self, board_shape=(6, 7), window_width=512, window_height=512):\n",
    "        super(ConnectFourEnv, self).__init__()\n",
    "\n",
    "        self.board_shape = board_shape\n",
    "\n",
    "        self.observation_space = spaces.Box(low=-1,\n",
    "                                            high=1,\n",
    "                                            shape=board_shape,\n",
    "                                            dtype=int)\n",
    "        self.action_space = spaces.Discrete(board_shape[1])\n",
    "\n",
    "        self.__current_player = 1\n",
    "        self.__board = np.zeros(self.board_shape, dtype=int)\n",
    "\n",
    "        self.__player_color = 1\n",
    "        self.__screen = None\n",
    "        self.__window_width = window_width\n",
    "        self.__window_height = window_height\n",
    "        self.__rendered_board = self._update_board_render()\n",
    "    \"\"\"\n",
    "    def run(self, player1: Player, player2: Player, board: Optional[np.ndarray] = None, render=False) -> ResultType:\n",
    "        player1.reset()\n",
    "        player2.reset()\n",
    "        self.reset(board)\n",
    "\n",
    "        cp = lambda: self.__current_player\n",
    "\n",
    "        def change_player():\n",
    "            self.__current_player *= -1\n",
    "            return player1 if cp() == 1 else player2\n",
    "\n",
    "        state_hist = deque([self.__board.copy()], maxlen=4)\n",
    "\n",
    "        act = player1.get_next_action(self.__board * 1)\n",
    "        act_hist = deque([act], maxlen=2)\n",
    "        step_result = self._step(act)\n",
    "        state_hist.append(self.__board.copy())\n",
    "        player = change_player()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if render:\n",
    "                self.render()\n",
    "            act_hist.append(player.get_next_action(self.__board * cp()))\n",
    "            step_result = self._step(act_hist[-1])\n",
    "            state_hist.append(self.__board.copy())\n",
    "\n",
    "            player = change_player()\n",
    "\n",
    "            reward = step_result.get_reward(cp())\n",
    "            done = step_result.is_done()\n",
    "            player.learn(state=state_hist[-3] * cp(), action=act_hist[-2], state_next=state_hist[-1] * cp(), reward=reward, done=done)\n",
    "\n",
    "        player = change_player()\n",
    "        reward = step_result.get_reward(cp())\n",
    "        player.learn(state_hist[-2] * cp(), act_hist[-1], state_hist[-1] * cp(), reward, done)\n",
    "        if render:\n",
    "            self.render()\n",
    "\n",
    "        return step_result.res_type\n",
    "\t\t\t\t\"\"\"\n",
    "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, dict]:\n",
    "        step_result = self._step(action)\n",
    "        reward = step_result.get_reward(self.__current_player)\n",
    "        done = step_result.is_done()\n",
    "        return self.__board.copy(), reward, done, {}\n",
    "\n",
    "    def _step(self, action: int) -> StepResult:\n",
    "        result = ResultType.NONE\n",
    "\n",
    "        if not self.is_valid_action(action):\n",
    "            #raise Exception(\n",
    "            #    'Unable to determine a valid move! Maybe invoke at the wrong time?'\n",
    "            #)\n",
    "            print('invalid move, return reward')\n",
    "        \n",
    "\n",
    "        # Check and perform action\n",
    "        for index in list(reversed(range(self.board_shape[0]))):\n",
    "            if self.__board[index][action] == 0:\n",
    "                self.__board[index][action] = self.__current_player\n",
    "                break\n",
    "\n",
    "        # Check if board is completely filled\n",
    "        if np.count_nonzero(self.__board[0]) == self.board_shape[1]:\n",
    "            result = ResultType.DRAW\n",
    "        else:\n",
    "            # Check win condition\n",
    "            if self.is_win_state():\n",
    "                result = ResultType.WIN1 if self.__current_player == 1 else ResultType.WIN2\n",
    "        self.__current_player *= -1\n",
    "        return self.StepResult(result)\n",
    "\n",
    "    @property\n",
    "    def board(self):\n",
    "        return self.__board.copy()\n",
    "\n",
    "    def reset(self, board: Optional[np.ndarray] = None) -> np.ndarray:\n",
    "        self.__current_player = 1\n",
    "        if board is None:\n",
    "            self.__board = np.zeros(self.board_shape, dtype=int)\n",
    "        else:\n",
    "            self.__board = board\n",
    "        self.__rendered_board = self._update_board_render()\n",
    "        return self.board\n",
    "\n",
    "    def render(self, mode: str = 'console', close: bool = False) -> None:\n",
    "        if mode == 'console':\n",
    "            replacements = {\n",
    "                self.__player_color: 'A',\n",
    "                0: ' ',\n",
    "                -1 * self.__player_color: 'B'\n",
    "            }\n",
    "\n",
    "            def render_line(line):\n",
    "                return \"|\" + \"|\".join(\n",
    "                    [\"{:>2} \".format(replacements[x]) for x in line]) + \"|\"\n",
    "\n",
    "            hline = '|---+---+---+---+---+---+---|'\n",
    "            print(hline)\n",
    "            for line in np.apply_along_axis(render_line,\n",
    "                                            axis=1,\n",
    "                                            arr=self.__board):\n",
    "                print(line)\n",
    "            print(hline)\n",
    "\n",
    "        elif mode == 'human':\n",
    "            if self.__screen is None:\n",
    "                pygame.init()\n",
    "                self.__screen = pygame.display.set_mode(\n",
    "                    (round(self.__window_width), round(self.__window_height)))\n",
    "\n",
    "            if close:\n",
    "                pygame.quit()\n",
    "\n",
    "            self.__rendered_board = self._update_board_render()\n",
    "            frame = self.__rendered_board\n",
    "            surface = pygame.surfarray.make_surface(frame)\n",
    "            surface = pygame.transform.rotate(surface, 90)\n",
    "            self.__screen.blit(surface, (0, 0))\n",
    "\n",
    "            pygame.display.update()\n",
    "        else:\n",
    "            raise error.UnsupportedMode()\n",
    "\n",
    "    def close(self) -> None:\n",
    "        pygame.quit()\n",
    "\n",
    "    def is_valid_action(self, action: int) -> bool:\n",
    "        return self.__board[0][action] == 0\n",
    "\n",
    "    def _update_board_render(self) -> np.ndarray:\n",
    "        return render_board(self.__board,\n",
    "                            image_width=self.__window_width,\n",
    "                            image_height=self.__window_height)\n",
    "\n",
    "    def is_win_state(self) -> bool:\n",
    "        # Test rows\n",
    "        for i in range(self.board_shape[0]):\n",
    "            for j in range(self.board_shape[1] - 3):\n",
    "                value = sum(self.__board[i][j:j + 4])\n",
    "                if abs(value) == 4:\n",
    "                    return True\n",
    "\n",
    "        # Test columns on transpose array\n",
    "        reversed_board = [list(i) for i in zip(*self.__board)]\n",
    "        for i in range(self.board_shape[1]):\n",
    "            for j in range(self.board_shape[0] - 3):\n",
    "                value = sum(reversed_board[i][j:j + 4])\n",
    "                if abs(value) == 4:\n",
    "                    return True\n",
    "\n",
    "        # Test diagonal\n",
    "        for i in range(self.board_shape[0] - 3):\n",
    "            for j in range(self.board_shape[1] - 3):\n",
    "                value = 0\n",
    "                for k in range(4):\n",
    "                    value += self.__board[i + k][j + k]\n",
    "                    if abs(value) == 4:\n",
    "                        return True\n",
    "\n",
    "        reversed_board = np.fliplr(self.__board)\n",
    "        # Test reverse diagonal\n",
    "        for i in range(self.board_shape[0] - 3):\n",
    "            for j in range(self.board_shape[1] - 3):\n",
    "                value = 0\n",
    "                for k in range(4):\n",
    "                    value += reversed_board[i + k][j + k]\n",
    "                    if abs(value) == 4:\n",
    "                        return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def available_moves(self) -> frozenset:\n",
    "        return frozenset(\n",
    "            (i for i in range(self.board_shape[1]) if self.is_valid_action(i)))\n",
    "    \n",
    "    def game_over(self):\n",
    "        if np.count_nonzero(self.__board[0]) == self.board_shape[1]:\n",
    "            return True\n",
    "        return self.is_win_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d248136a",
   "metadata": {},
   "source": [
    "La classe node est la classe qui est utilisée pour représenter les différents nœuds de l’arbre du MCTS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8742f74f",
   "metadata": {
    "id": "8742f74f"
   },
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "\n",
    "    def __init__(self, parent, probability, state=None, action=None):\n",
    "        #we use state = none for all nodes that aren't root to avoid creating too much instance of state\n",
    "        self.state = state\n",
    "        self.numbervisits = 0\n",
    "        #Q = average of estimated outcome for all the children of the node\n",
    "        self.Q = 0\n",
    "        #U = node value to assure exploration (see : https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ page8)\n",
    "        self.U = 0\n",
    "        self.moveprobability = probability\n",
    "        self.parent = parent\n",
    "        self.children = {}\n",
    "        self.action = action\n",
    "\n",
    "\n",
    "    def updateU(self) :\n",
    "        #cpuct\n",
    "        csteExpl = 2\n",
    "        self.U = csteExpl * self.moveprobability * np.sqrt(self.parent.numbervisits) / (1 + self.numbervisits)\n",
    "\n",
    "    \n",
    "    def updateNode(self, value):\n",
    "        self.numbervisits += 1\n",
    "        self.Q += (value - self.Q) / self.numbervisits\n",
    "        \n",
    "    def backFill(self,value):\n",
    "        #we will use this function to update all the parents node\n",
    "        if self.parent != None :\n",
    "            #we take the opposite of the value for every parents as the parent represent the opposing player\n",
    "            self.parent.backFill(-value)\n",
    "        self.updateNode(value)\n",
    "        \n",
    "    \n",
    "    def addChildren(self,NN_move_probability) :\n",
    "        for i in range(len(NN_move_probability)):\n",
    "            if NN_move_probability[i] > 0 : \n",
    "                newNode = Node(self,NN_move_probability[i], action=i)\n",
    "                self.children[i] = newNode\n",
    "            \n",
    "            \n",
    "    def moveToLeaf(self) :\n",
    "        \n",
    "        currentNode = self\n",
    "        current_state = copy.deepcopy(self.state)\n",
    "        i=0\n",
    "        #while we are not on a leaf :\n",
    "        while currentNode.children != {}:\n",
    "            #update this to add noise see : https://drive.google.com/drive/folders/1qFpWV_xuGIPKwK0cFV7yzJg2glx825in\n",
    "            # Acquisition of Chess Knowledge in AlphaZero page 8\n",
    "            scoreMax = float('-inf')\n",
    "            nodeMax = self\n",
    "            i += 1\n",
    "            #we iterate over all the children and chose the one with max score Q + U\n",
    "            for node in currentNode.children.values():\n",
    "                node.updateU()\n",
    "                score = node.Q + node.U\n",
    "                if score > scoreMax:\n",
    "                    scoreMax = score\n",
    "                    nodeMax = node\n",
    "                #print to see how the tree works\n",
    "                #print(f'child {i} score Q : {node.Q} and U : {node.U} action :  {node.action}')\n",
    "            currentNode = nodeMax\n",
    "            #print(f'current node action (node max) : {currentNode.action}')\n",
    "            #if we get to the end of the game in a simulation we use the true reward for the MCTS\n",
    "            if currentNode.action != None:\n",
    "                current_state._step(currentNode.action)\n",
    "        currentNode.state = current_state\n",
    "        return currentNode\n",
    "                \n",
    "    def isLeaf(self):\n",
    "        if len(self.edges) > 0:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb623df6",
   "metadata": {},
   "source": [
    "La classe MCTS est la classe utilisée pour effectuer la recherche arborescente Monte Carlo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "21f8158b",
   "metadata": {
    "id": "21f8158b"
   },
   "outputs": [],
   "source": [
    "class MCTS():\n",
    "    \"\"\"this class will be used to run the MCTS algorithm\"\"\"\n",
    "    def __init__(self, NeuralNetwork, rootNode):\n",
    "        self.NN = NeuralNetwork\n",
    "        self.root = rootNode\n",
    "        \n",
    "        \n",
    "    def oneIteration(self):\n",
    "        #we use the moveToLeaf method to go to the node leaf node based on the scores of every nodes\n",
    "        leafNode = self.root.moveToLeaf()\n",
    "        #if the game is over we use  the true reward\n",
    "        if leafNode.state.is_win_state():\n",
    "            value = 1\n",
    "            #print(f'value early winner {value}')\n",
    "            leafNode.backFill(value)\n",
    "            return value\n",
    "        #we get p and v from our NN\n",
    "        #start2 = time.time()\n",
    "        probabilities, value = self.NN.predict([ leafNode.state._ConnectFourEnv__board.reshape(1,6,7) * leafNode.state._ConnectFourEnv__current_player ])\n",
    "        #our NN tell us if the board is winning for player2 = -1 (value -1) or for player1 = 1\n",
    "        #end2 = time.time()\n",
    "        #print(f'time NEURAL NETWORK {end2 - start2}')\n",
    "        #add new children\n",
    "        legalMoves = list(leafNode.state.available_moves())\n",
    "        legalMovesMask = [0 if i not in legalMoves else 1 for i in range(7)]\n",
    "        probabilities = probabilities[0]  * legalMovesMask\n",
    "        probabilities /= np.sum(probabilities)\n",
    "        #we create new nodes from our leaf\n",
    "        leafNode.addChildren(probabilities)\n",
    "        leafNode.backFill(value)\n",
    "        return value\n",
    "\n",
    "    \n",
    "    def selectAction(self):\n",
    "        maxNode = None\n",
    "        maxN = 0\n",
    "        for children in self.root.children:\n",
    "                if maxN < children.numbervisits:\n",
    "                    maxN = children.numbervisits\n",
    "                    maxNode = children\n",
    "        return children, children.action\n",
    "    \n",
    "    def nIterations(self, nb_of_iteration):\n",
    "        \n",
    "        for i in range(nb_of_iteration):\n",
    "            self.oneIteration()\n",
    "        probabilities = {}\n",
    "        sum_numberofvisit = 0\n",
    "        for children in self.root.children.values():\n",
    "            sum_numberofvisit += children.numbervisits\n",
    "        for children in self.root.children.values():\n",
    "            probabilities[children.action] = children.numbervisits / sum_numberofvisit\n",
    "        #print(f'probabilities of move : {probabilities}')\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "539c3ac8",
   "metadata": {
    "id": "539c3ac8"
   },
   "outputs": [],
   "source": [
    "def softmax(p):\n",
    "    s = np.exp(p) / np.sum(np.exp(p))\n",
    "    return s\n",
    "\n",
    "\n",
    "def modelAgainstModel(model1, model2, numberOfGames):\n",
    "    \"\"\"function used to make 2 models play against each others\"\"\"\n",
    "    winRatePlayer1 = 0\n",
    "    for i in range(numberOfGames):\n",
    "        env =  ConnectFourEnv()\n",
    "        GameOver = False\n",
    "            #we create the game\n",
    "        env.reset()\n",
    "        while not GameOver:\n",
    "\n",
    "            currentStateNode = Node(None, 0, state=env, action=None)\n",
    "            mcts1 = MCTS(model1 ,currentStateNode)\n",
    "            mcts2 = MCTS(model2 ,currentStateNode)\n",
    "            if env._ConnectFourEnv__current_player == 1 :\n",
    "                probabilitiesnodemcts = mcts1.nIterations(75)\n",
    "            else:\n",
    "                probabilitiesnodemcts = mcts2.nIterations(75)            \n",
    "            probabilities = [0 for i in range(7)]\n",
    "            for move in probabilitiesnodemcts.keys():\n",
    "                probabilities[move] = probabilitiesnodemcts[move]\n",
    "            action = np.random.choice(list(probabilitiesnodemcts.keys()), p=softmax(list(probabilitiesnodemcts.values())))\n",
    "            env._step(action)\n",
    "            GameOver = env.game_over()\n",
    "            print(f'probabilities of doing each moves : {probabilities}')\n",
    "            print(np.array(env._ConnectFourEnv__board))\n",
    "        if 1 == -1 * env._ConnectFourEnv__current_player:\n",
    "            winRatePlayer1 += 1\n",
    "        print(f'gagnant :  {-1 * env._ConnectFourEnv__current_player}')\n",
    "    winRatePlayer1 = winRatePlayer1 / numberOfGames\n",
    "    return winRatePlayer1 \n",
    "\n",
    "\n",
    "class Agent():\n",
    "    \n",
    "    def __init__(self, input_shape, output_shape, model=None):\n",
    "        if model==None:\n",
    "            self.neuralNetwork = createModelSmall(input_shape, output_shape, l2const =0.2)\n",
    "        else:\n",
    "            self.neuralNetwork = model\n",
    "        \n",
    "    def generateData(self, gamesPerBundle):\n",
    "        \n",
    "        statesBatch = []\n",
    "        policiesBatch = []\n",
    "        valuesBatch = []\n",
    "        model = self.neuralNetwork\n",
    "        env =  ConnectFourEnv() \n",
    "        for i in range(gamesPerBundle):\n",
    "            print(f'Game {i} in progress')\n",
    "            GameOver = False\n",
    "            #we create the game\n",
    "            env.reset()\n",
    "            valuesBatchTemp = []\n",
    "            a = 0\n",
    "\n",
    "            while not GameOver:\n",
    "\n",
    "                currentStateNode = Node(None, 0, state=env, action=None)\n",
    "                mcts = MCTS(model ,currentStateNode)\n",
    "                probabilitiesnodemcts = mcts.nIterations(50)\n",
    "                #reward = -1 * env._ConnectFourEnv__current_player\n",
    "                #we don't take every position because \"positions on subsequent moves are strongly correlated, \n",
    "                #and including all of them may lead to increased overfitting.\"\n",
    "                if np.random.rand(1)[0] < 1:\n",
    "                    statesBatch.append(env._ConnectFourEnv__board.reshape(6,7,1) * env._ConnectFourEnv__current_player)\n",
    "                    probabilities = [0 for i in range(7)]\n",
    "                    for move in probabilitiesnodemcts.keys():\n",
    "                         probabilities[move] = probabilitiesnodemcts[move]\n",
    "                    policiesBatch.append(probabilities)\n",
    "                    #we use this to remember if it was black or a white move\n",
    "                    valuesBatchTemp.append(-1 * env._ConnectFourEnv__current_player)\n",
    "                #we check if the game is over\n",
    "                #print(f'turn : {a} board : {np.array(env._ConnectFourEnv__board) * env._ConnectFourEnv__current_player}')\n",
    "                #print(f'probabilities : {probabilities}')\n",
    "                action = np.random.choice(list(probabilitiesnodemcts.keys()), p=softmax(list(probabilitiesnodemcts.values())))\n",
    "                #print(f'action:  {action}')\n",
    "                env._step(action)\n",
    "                a+=1\n",
    "                GameOver = env.game_over()\n",
    "            #once the game is over we check the reward\n",
    "            valuesBatchTemp = np.array(valuesBatchTemp)\n",
    "            valuesBatch.extend(valuesBatchTemp)\n",
    "        return (statesBatch, policiesBatch, valuesBatch)\n",
    "    \n",
    "    def Train(self, statesBatch, policiesBatch, valuesBatch):\n",
    "        self.neuralNetwork.fit(np.array(statesBatch), [np.array(policiesBatch), np.array(valuesBatch)],epochs=2,batch_size=32)\n",
    "        \n",
    "    def selfPlay(self, gamesPerBundle):\n",
    "        return True\n",
    "        \n",
    "    def saveModel(self, path):\n",
    "        cwd = os.getcwd() \n",
    "        self.neuralNetwork.save(cwd + '/modelSaved' + path)\n",
    "\n",
    "    def loadModel(self, path):\n",
    "        \"\"\"path = 'path/to/location' \"\"\"\n",
    "        self.neuralNetwork =  keras.models.load_model(path)\n",
    "        return keras.models.load_model(path)\n",
    "        \n",
    "    def train(self, gamesPerBundle, nbCheckpoint):\n",
    "        for checkpoint in range(nbCheckpoint):\n",
    "            statesBatch, policiesBatch, valuesBatch = self.generateData(gamesPerBundle)\n",
    "            self.Train(statesBatch, policiesBatch, valuesBatch)\n",
    "            np.save('statesBatchSaved'+ f'{checkpoint}', statesBatch)\n",
    "            np.save('policiesBatchSaved' + f'{checkpoint}', policiesBatch)\n",
    "            np.save('valuesBatchSaved'+ f'{checkpoint}', valuesBatch)\n",
    "            self.saveModel(f'{checkpoint}')\n",
    "        return True\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "55d06fc1",
   "metadata": {
    "id": "55d06fc1"
   },
   "outputs": [],
   "source": [
    "def createModelSmall(input_shape ,output_shape, l2const):\n",
    "    input = Input(input_shape)\n",
    "    layer = Conv2D(75, (3,3), use_bias=False, activation='relu',padding='same',strides=1)(input)\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    #smaller version of the deep resnet used in alpha zero :\n",
    "    for i in range(3):\n",
    "        res_layer = layer\n",
    "        layer = keras.layers.Conv2D(75, (4, 4), kernel_regularizer = keras.regularizers.l2(l2const),padding='same',strides=1)(layer)\n",
    "        layer = keras.layers.Activation(\"relu\")(layer)\n",
    "        layer = keras.layers.Conv2D(75, (2, 2), kernel_regularizer = keras.regularizers.l2(l2const),padding='same',strides=1)(layer)\n",
    "        layer = keras.layers.BatchNormalization()(layer)\n",
    "        layer = keras.layers.Add()([layer, res_layer])\n",
    "        layer = keras.layers.Activation(\"relu\")(layer)\n",
    "    \n",
    "    value_layer = layer\n",
    "    value_layer = Conv2D(1, (1,1))(value_layer)\n",
    "    value_layer = BatchNormalization()(value_layer)\n",
    "    value_layer = Flatten()(value_layer)\n",
    "    value_layer = Activation('relu')(value_layer)\n",
    "    value_layer = Dense(16, activation='relu')(value_layer)\n",
    "    value_layer = Dense(1, activation='tanh', name = 'value')(value_layer)\n",
    "\n",
    "    policy_layer = layer\n",
    "    policy_layer = Conv2D(2, (1,1),padding='same',strides=1)(policy_layer)\n",
    "    policy_layer = BatchNormalization()(policy_layer)\n",
    "    policy_layer = Flatten()(policy_layer)\n",
    "    policy_layer = Activation(\"relu\")(policy_layer)\n",
    "    policy_layer = Dense(7)(policy_layer)\n",
    "    policy_layer = keras.layers.Activation(\"softmax\", name = \"policy\")(policy_layer)\n",
    "\n",
    "    model = keras.models.Model(inputs = [input], outputs = [policy_layer, value_layer])\n",
    "    model.compile(\n",
    "            optimizer = Adam(),\n",
    "            loss = [keras.losses.categorical_crossentropy, keras.losses.mean_squared_error],\n",
    "            loss_weights = [0.5, 0.5],\n",
    "            metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def createModel(input_shape, output_shape, l2const):\n",
    "    input = Input(input_shape)\n",
    "    layer = Conv2D(64, (3,3), use_bias=False, input_shape=input_shape,padding='same',strides=1)(input)\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    #smaller version of the deep resnet used in alpha zero :\n",
    "    for i in range(4):\n",
    "        print(i)\n",
    "        res_layer = layer\n",
    "        layer = keras.layers.Conv2D(64, (4, 4), kernel_regularizer = keras.regularizers.l2(l2const),padding='same',strides=1)(layer)\n",
    "        layer = keras.layers.Activation(\"relu\")(layer)\n",
    "        layer = keras.layers.Conv2D(64, (2, 2), kernel_regularizer = keras.regularizers.l2(l2const),padding='same',strides=1)(layer)\n",
    "        layer = keras.layers.BatchNormalization()(layer)\n",
    "        layer = keras.layers.Add()([layer, res_layer])\n",
    "        layer = keras.layers.Activation(\"relu\")(layer)\n",
    "    \n",
    "    value_layer = layer\n",
    "    value_layer = Conv2D(1, (1,1))(value_layer)\n",
    "    value_layer = BatchNormalization()(value_layer)\n",
    "    value_layer = Flatten()(value_layer)\n",
    "    value_layer = Activation('relu')(value_layer)\n",
    "    value_layer = Dense(128, activation='relu')(value_layer)\n",
    "    value_layer = Dense(1, activation='tanh', name = 'value')(value_layer)\n",
    "\n",
    "    policy_layer = layer\n",
    "    policy_layer = Conv2D(output_shape, (1,1),padding='same',strides=1)(policy_layer)\n",
    "    policy_layer = BatchNormalization()(policy_layer)\n",
    "    policy_layer = Activation(\"relu\")(policy_layer)\n",
    "    policy_layer = keras.layers.Flatten()(policy_layer)\n",
    "    policy_layer = keras.layers.Activation(\"softmax\", name = \"policy\")(policy_layer)\n",
    "\n",
    "    model = keras.models.Model(inputs = [input], outputs = [policy_layer, value_layer])\n",
    "    model.compile(\n",
    "            optimizer = Adam(),\n",
    "            loss = [keras.losses.categorical_crossentropy, keras.losses.mean_squared_error],\n",
    "            loss_weights = [0.5, 0.5],\n",
    "            metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "34b24701",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34b24701",
    "outputId": "4182419a-db83-416d-d2e9-ec81f165d63a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_32\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_33 (InputLayer)          [(None, 6, 7, 1)]    0           []                               \n",
      "                                                                                                  \n",
      " conv2d_476 (Conv2D)            (None, 6, 7, 75)     675         ['input_33[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_286 (Batch  (None, 6, 7, 75)    300         ['conv2d_476[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " activation_476 (Activation)    (None, 6, 7, 75)     0           ['batch_normalization_286[0][0]']\n",
      "                                                                                                  \n",
      " conv2d_477 (Conv2D)            (None, 6, 7, 75)     90075       ['activation_476[0][0]']         \n",
      "                                                                                                  \n",
      " activation_477 (Activation)    (None, 6, 7, 75)     0           ['conv2d_477[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_478 (Conv2D)            (None, 6, 7, 75)     22575       ['activation_477[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_287 (Batch  (None, 6, 7, 75)    300         ['conv2d_478[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_190 (Add)                  (None, 6, 7, 75)     0           ['batch_normalization_287[0][0]',\n",
      "                                                                  'activation_476[0][0]']         \n",
      "                                                                                                  \n",
      " activation_478 (Activation)    (None, 6, 7, 75)     0           ['add_190[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_479 (Conv2D)            (None, 6, 7, 75)     90075       ['activation_478[0][0]']         \n",
      "                                                                                                  \n",
      " activation_479 (Activation)    (None, 6, 7, 75)     0           ['conv2d_479[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_480 (Conv2D)            (None, 6, 7, 75)     22575       ['activation_479[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_288 (Batch  (None, 6, 7, 75)    300         ['conv2d_480[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_191 (Add)                  (None, 6, 7, 75)     0           ['batch_normalization_288[0][0]',\n",
      "                                                                  'activation_478[0][0]']         \n",
      "                                                                                                  \n",
      " activation_480 (Activation)    (None, 6, 7, 75)     0           ['add_191[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_481 (Conv2D)            (None, 6, 7, 75)     90075       ['activation_480[0][0]']         \n",
      "                                                                                                  \n",
      " activation_481 (Activation)    (None, 6, 7, 75)     0           ['conv2d_481[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_482 (Conv2D)            (None, 6, 7, 75)     22575       ['activation_481[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_289 (Batch  (None, 6, 7, 75)    300         ['conv2d_482[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " add_192 (Add)                  (None, 6, 7, 75)     0           ['batch_normalization_289[0][0]',\n",
      "                                                                  'activation_480[0][0]']         \n",
      "                                                                                                  \n",
      " activation_482 (Activation)    (None, 6, 7, 75)     0           ['add_192[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_484 (Conv2D)            (None, 6, 7, 2)      152         ['activation_482[0][0]']         \n",
      "                                                                                                  \n",
      " conv2d_483 (Conv2D)            (None, 6, 7, 1)      76          ['activation_482[0][0]']         \n",
      "                                                                                                  \n",
      " batch_normalization_291 (Batch  (None, 6, 7, 2)     8           ['conv2d_484[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " batch_normalization_290 (Batch  (None, 6, 7, 1)     4           ['conv2d_483[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " flatten_65 (Flatten)           (None, 84)           0           ['batch_normalization_291[0][0]']\n",
      "                                                                                                  \n",
      " flatten_64 (Flatten)           (None, 42)           0           ['batch_normalization_290[0][0]']\n",
      "                                                                                                  \n",
      " activation_484 (Activation)    (None, 84)           0           ['flatten_65[0][0]']             \n",
      "                                                                                                  \n",
      " activation_483 (Activation)    (None, 42)           0           ['flatten_64[0][0]']             \n",
      "                                                                                                  \n",
      " dense_65 (Dense)               (None, 7)            595         ['activation_484[0][0]']         \n",
      "                                                                                                  \n",
      " dense_64 (Dense)               (None, 16)           688         ['activation_483[0][0]']         \n",
      "                                                                                                  \n",
      " policy (Activation)            (None, 7)            0           ['dense_65[0][0]']               \n",
      "                                                                                                  \n",
      " value (Dense)                  (None, 1)            17          ['dense_64[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 341,365\n",
      "Trainable params: 340,759\n",
      "Non-trainable params: 606\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent = Agent((6,7,1), 7)\n",
    "agent.neuralNetwork.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "fe85b884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.vis_utils.plot_model(agent.neuralNetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0MJSnoefqlHE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0MJSnoefqlHE",
    "outputId": "e3e7ebe0-e24d-47f2-8e7c-94088b73a2f0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#use this to import agent, then load the chosen model. If no model are chosen the agent will have default parameters for his NN(close to classic MCTS with no NN)\n",
    "agent2 = Agent((6,7,1), 7)\n",
    "agent1 = Agent((6,7,1), 7)\n",
    "cwd = os.getcwd()\n",
    "#agent1.neuralNetwork = keras.models.load_model(cwd + '/bestModel')\n",
    "agent2.neuralNetwork = keras.models.load_model(cwd + '/bestModel')\n",
    "#modelAgainstModel(model1,model2,nbOfGames)\n",
    "winRatePlayer1 = modelAgainstModel(agent1.neuralNetwork, agent2.neuralNetwork,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "f7a30796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winRatePlayer1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68036e1b",
   "metadata": {},
   "source": [
    "Use to train :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a884a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent.train(60,1)\n",
    "#parameter 1 = nb of game per update\n",
    "#parameter 2 : number of time we will play parameter1 games and update"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "alphazero-connect4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
